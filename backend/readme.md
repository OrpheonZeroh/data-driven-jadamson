# Business-Driven Data Portfolio
## Retail â€¢ Airlines â€¢ Telco Churn â€¢ Fraud Detection

Portafolio completo de analÃ­tica orientada a negocio para roles de estrategia/BI/FP&A/Revenue/Retention/Risk.  
Incluye: limpieza + modelado + KPIs + dashboards ejecutivos + recomendaciones accionables + ML fraud detection.

## ğŸ¯ Objetivo
Demostrar capacidad para:
- convertir datasets en **mÃ©tricas de negocio** (ARPU, Churn, CAC, LTV, Gross Margin, eficiencia)
- construir **dashboards ejecutivos**
- generar **insights accionables** y **casos de negocio**
- publicar una app con UX clara (Next.js) consumiendo un backend analÃ­tico en Python

---

## ğŸ“¦ Datasets

| Dataset | Registros | Objetivo de Negocio | Estado |
|---------|-----------|---------------------|--------|
| **Retail Sales** | 1,000 | Optimizar mix y rentabilidad | âœ… Procesado |
| **Airlines Flights** | 300,153 | Eficiencia operativa y pricing | âœ… Procesado |
| **Telco Customer Churn** | 7,043 | Reducir churn y maximizar CLV | âœ… Procesado |
| **Fraud Detection** | 7.48M â†’ 1.69M | Detectar transacciones fraudulentas | âœ… Compactado |

---

## ğŸ§± Arquitectura (Next.js + Python backend)
### Frontend (Next.js)
- App con **3 tabs**: Retail, Airlines, Telco
- Un tab adicional opcional: **Overview (Executive)** con tarjetas globales y comparativos
- Componentes:
  - KPI Cards (variaciÃ³n, meta, semÃ¡foro)
  - Charts dinÃ¡micos (filtros: fecha, regiÃ³n, categorÃ­a, canal, segmento)
  - Tabla drill-down con export CSV

### Backend (Python)
- API en **FastAPI**
- Procesamiento:
  - `pandas/polars` para ETL
  - modelos/estadÃ­stica cuando aplique (churn propensity, forecast simple)
- Respuestas listas para UI (series para charts, KPIs agregados, breakdowns)

### Flujo
1) Usuario filtra en UI â†’ 2) Next.js llama API â†’ 3) API agrega/filtra â†’ 4) UI renderiza

---

## ğŸ—‚ï¸ Estructura del repo
.
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ web/                   # Next.js frontend
â”‚   â””â”€â”€ api/                   # FastAPI backend
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # CSV originales
â”‚   â””â”€â”€ processed/             # datasets limpios/featureados (parquet recomendado)
â”œâ”€â”€ notebooks/                 # exploraciÃ³n y prototipos (opcional)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ executive_summary.md   # resumen ejecutivo (1-2 pÃ¡ginas)
â”‚   â””â”€â”€ data_dictionary.md     # diccionario de datos y definiciones KPI
â””â”€â”€ docker-compose.yml         # levantar web + api

---

# âœ… Entregables (por dataset)

## 1) Retail Sales â€” â€œRentabilidad & Mixâ€
### Preguntas de negocio
- Â¿QuÃ© productos/categorÃ­as destruyen margen?
- Â¿QuÃ© regiones/canales son mÃ¡s rentables?
- Â¿DÃ³nde enfocar inventario y promociones?

### KPIs principales (Cards)
- Revenue total
- Gross Margin % (si hay COGS; si no, proxy/markup)
- Ticket promedio (AOV)
- Top categorÃ­as por contribuciÃ³n
- Pareto 80/20 (contribuciÃ³n a revenue y/o margen)

### Visualizaciones recomendadas
- Serie temporal: Revenue y Gross Margin %
- Barras: Top categorÃ­as por contribuciÃ³n
- Pareto chart: contribuciÃ³n acumulada (SKU/Category)
- Heatmap: RegiÃ³n x CategorÃ­a (Revenue/Margin)
- Tabla drill-down: SKU/Category con filtros

### Insights esperados (outputs)
- Lista de â€œmargin killersâ€
- Segmentos de alta contribuciÃ³n vs bajo margen
- RecomendaciÃ³n de mix (quÃ© empujar / quÃ© retirar)

### Endpoints API (propuestos)
- GET `/retail/summary`
- GET `/retail/timeseries?metric=revenue&grain=month`
- GET `/retail/breakdown?by=category&metric=revenue`
- GET `/retail/pareto?by=sku&metric=revenue`
- GET `/retail/table?filters=...`

---

## 2) Airlines â€” â€œEficiencia Operativa & Confiabilidadâ€
### Preguntas de negocio
- Â¿QuÃ© rutas generan mÃ¡s retraso/costo?
- Â¿DÃ³nde estÃ¡n los cuellos de botella operativos?
- Â¿QuÃ© variables explican retrasos?

### KPIs principales (Cards)
- Total flights
- On-time rate (si hay delay)
- Avg delay (min)
- Top rutas con mayor delay
- â€œDelay impact scoreâ€ (mÃ©trica compuesta: delay * volumen)

### Visualizaciones recomendadas
- Time series: on-time / delay
- Ranking: rutas con peor desempeÃ±o
- Scatter: volumen vs delay promedio (priorizaciÃ³n)
- Heatmap: origen-destino vs delay
- Tabla: rutas + mÃ©tricas + tendencia

### Insights esperados
- Rutas â€œproblemaâ€ (alto volumen + alto delay)
- PriorizaciÃ³n de mejora (top 10 iniciativas)
- Narrativa de eficiencia y reducciÃ³n de costo (aunque sea proxy)

### Endpoints API (propuestos)
- GET `/airlines/summary`
- GET `/airlines/timeseries?metric=delay&grain=week`
- GET `/airlines/routes/rank?metric=avg_delay`
- GET `/airlines/routes/scatter?x=volume&y=avg_delay`
- GET `/airlines/table?filters=...`

---

## 3) Telco Churn â€” â€œRetenciÃ³n, ARPU y Valor del Clienteâ€
### Preguntas de negocio
- Â¿QuiÃ©n se estÃ¡ yendo y por quÃ©?
- Â¿CuÃ¡nto revenue se pierde por churn?
- Â¿QuÃ© palancas bajan churn sin erosionar ARPU?

### KPIs principales (Cards)
- Churn rate
- ARPU (MonthlyCharges avg)
- Revenue at risk (ARPU * churners)
- Segmentos crÃ­ticos (alto churn + alto ARPU)
- Drivers top (contrato, tenure, servicios)

### Visualizaciones recomendadas
- Funnel/stack: churn por segmento (contract type, tenure buckets)
- Cohorts: churn por tenure (bucket)
- Barras: churn rate por categorÃ­a
- SHAP/feature importance (opcional si hacemos modelo simple)
- Simulador: â€œsi reduzco churn 1pp => impacto $â€

### Insights esperados
- Segmentos con churn alto y acciones recomendadas
- RetenciÃ³n por cohortes (tenure)
- Caso de negocio: impacto financiero de reducciÃ³n de churn

### Endpoints API (propuestos)
- GET `/telco/summary`
- GET `/telco/churn/breakdown?by=contract`
- GET `/telco/churn/cohorts?by=tenure_bucket`
- GET `/telco/revenue_at_risk`
- POST `/telco/model/train` (opcional)
- GET `/telco/model/importance` (opcional)

---

# ğŸ§­ Tab â€œExecutive Overviewâ€ (recomendado)
Una vista para directivos con:
- Cards globales: Revenue, Gross Margin (Retail), On-time (Airlines), Churn/ARPU (Telco)
- â€œTop 3 alertsâ€ (anomalÃ­as y riesgos)
- Recomendaciones accionables (bullets)
- Un selector de dataset para drill-down

Endpoints:
- GET `/overview/summary`
- GET `/overview/alerts`
- GET `/overview/recommendations`

---

## ğŸ§ª MetodologÃ­a y estÃ¡ndares
### Data pipeline
1) Ingesta CSV â†’ 2) limpieza â†’ 3) estandarizaciÃ³n â†’ 4) features â†’ 5) outputs API
- Guardamos `processed/*.parquet` para performance.

### Calidad de datos (checks)
- nulos por columna
- valores fuera de rango
- duplicados
- consistencia de tipos
- â€œdata freshnessâ€ (si aplica)

---

## ğŸ› ï¸ Stack sugerido
Frontend:
- Next.js (App Router)
- Recharts (charts)
- TanStack Table (tabla)
- Zod (validaciÃ³n)
- Tailwind (UI)

Backend:
- FastAPI
- Pandas/Polars
- Pydantic
- Uvicorn
- (Opcional) scikit-learn para churn model

Persistencia:
- Archivos Parquet o SQLite/Postgres (si queremos histÃ³rico)

---

## ğŸš€ CÃ³mo correr local (propuesto)
### OpciÃ³n A: Docker Compose
- `docker-compose up --build`
- Frontend: `http://localhost:3000`
- API: `http://localhost:8000/docs`

### OpciÃ³n B: Manual
API:
- `cd apps/api`
- `pip install -r requirements.txt`
- `uvicorn main:app --reload --port 8000`

Web:
- `cd apps/web`
- `npm install`
- `npm run dev`

---

## ğŸ§  Roadmap
### Fase 1 (MVP)
- ETL + KPIs base + 3 tabs + overview
- 5-7 charts por dataset + tabla drill-down

### Fase 2
- Alertas automÃ¡ticas (anomalÃ­as simples)
- Modelo churn (baseline) + explicabilidad bÃ¡sica

### Fase 3
- Forecast (retail demand / telco churn) + simuladores
- Export reports (PDF) â€œboard-readyâ€

---

---

## ğŸ—„ï¸ MODELOS DE DATOS IMPLEMENTADOS

### **Arquitectura General**
```
RAW DATA (CSV)
    â†“
PROCESSING LAYER (Python/Pandas)
    â”œâ”€ Data Cleaning
    â”œâ”€ Feature Engineering
    â”œâ”€ KPI Calculation
    â”œâ”€ Data Compaction (Fraud)
    â””â”€ Data Quality Checks
    â†“
SUPABASE (PostgreSQL)
    â”œâ”€ Transaction Tables
    â”œâ”€ Aggregated KPIs
    â”œâ”€ Materialized Views
    â”œâ”€ Analytical Functions
    â””â”€ Data Validation
    â†“
DASHBOARDS & ML MODELS
```

---

## ğŸ“Š 1. RETAIL SALES - Modelo de Datos

### **Tablas Implementadas**

#### `retail_transactions` (1,000 registros)
**PropÃ³sito**: Transacciones individuales con mÃ©tricas de rentabilidad

| Campo | Tipo | CÃ¡lculo/Origen | DescripciÃ³n |
|-------|------|----------------|-------------|
| transaction_id | TEXT | Original | ID Ãºnico de transacciÃ³n |
| date | DATE | Original | Fecha de compra |
| customer_id | TEXT | Original | ID del cliente |
| product_category | TEXT | Original | CategorÃ­a (Beauty, Clothing, Electronics) |
| quantity | INT | Original | Unidades compradas |
| price_per_unit | DECIMAL | Original | Precio unitario |
| total_amount | DECIMAL | Original | Revenue de la transacciÃ³n |
| **total_cogs** | DECIMAL | `Price Ã— 0.60 Ã— Quantity` | Costo de bienes (60% markup) |
| **gross_profit** | DECIMAL | `Total Amount - Total COGS` | Ganancia bruta |
| **gross_margin_pct** | DECIMAL | `(Gross Profit / Total Amount) Ã— 100` | Margen bruto % |

#### `retail_monthly_kpis` (36 registros)
**PropÃ³sito**: KPIs agregados por mes y categorÃ­a

**Algoritmo de AgregaciÃ³n**:
```python
monthly_kpis = df.groupby(['YearMonth', 'Product Category']).agg({
    'Total Amount': 'sum',           # Revenue total
    'Gross_Profit': 'sum',           # Profit total
    'Transaction ID': 'count',       # NÃºmero de transacciones
    'Quantity': 'sum'                # Unidades vendidas
})
monthly_kpis['margin_pct'] = (profit / revenue) Ã— 100
```

### **KPIs Calculados**
- **Revenue Total**: $456,000
- **Gross Margin**: 40% (consistente)
- **AOV (Average Order Value)**: `Revenue / Transacciones` = $456
- **Pareto 80/20**: Electronics 34.4%, Clothing 34.1%, Beauty 31.5%

---

## âœˆï¸ 2. AIRLINES FLIGHTS - Modelo de Datos

### **Tablas Implementadas**

#### `airlines_flights` (300,153 registros)
**PropÃ³sito**: Vuelos individuales con mÃ©tricas de eficiencia

| Campo | Tipo | CÃ¡lculo/Origen | DescripciÃ³n |
|-------|------|----------------|-------------|
| airline | TEXT | Original | AerolÃ­nea (Vistara, Air India, etc.) |
| route | TEXT | `Source â†’ Destination` | Ruta completa |
| duration | DECIMAL | Original | DuraciÃ³n en horas |
| price | DECIMAL | Original | Precio del boleto |
| stops | TEXT | Original | Escalas (zero, one, two_or_more) |
| class | TEXT | Original | Clase (Economy, Business) |
| **price_per_hour** | DECIMAL | `Price / Duration` | Eficiencia de precio |
| **flight_length** | TEXT | Bins de duration | Short/Medium/Long/Very Long |
| **booking_window** | TEXT | Bins de days_left | Last Minute/Short/Medium/Early |

#### `airlines_route_kpis` (180 registros)
**PropÃ³sito**: KPIs por ruta y aerolÃ­nea

**Algoritmo de AgregaciÃ³n**:
```python
route_kpis = df.groupby(['route', 'airline']).agg({
    'price': ['mean', 'sum', 'count'],
    'duration': 'mean',
    'stops': lambda x: (x == 'zero').sum()
})
route_kpis['direct_flight_rate'] = (direct_flights / total_flights) Ã— 100
route_kpis['efficiency_score'] = direct_flight_rate / avg_duration
```

### **KPIs Calculados**
- **Direct Flight Rate**: 12% (bajo - oportunidad de mejora)
- **Average Price**: $20,890
- **Business Class Revenue**: 78% del total (alta dependencia)
- **Efficiency Score**: Ranking de rutas por eficiencia operativa

---

## ğŸ“ 3. TELCO CHURN - Modelo de Datos

### **Tablas Implementadas**

#### `telco_customers` (7,043 registros)
**PropÃ³sito**: Perfiles de clientes con anÃ¡lisis de churn

| Campo | Tipo | CÃ¡lculo/Origen | DescripciÃ³n |
|-------|------|----------------|-------------|
| customer_id | TEXT | Original | ID Ãºnico del cliente |
| tenure | INT | Original (limpio) | Meses como cliente |
| monthly_charges | DECIMAL | Original | ARPU (cargo mensual) |
| total_charges | DECIMAL | Cleaned (fillna) | Cargos acumulados |
| churn | TEXT | Original | Hizo churn (Yes/No) |
| **churn_binary** | INT | `1 if Churn='Yes' else 0` | Variable objetivo para ML |
| **tenure_segment** | TEXT | Bins de tenure | 0-12/13-24/25-48/48+ meses |
| **arpu_segment** | TEXT | Bins de monthly_charges | Low/Medium/High/Premium |
| **estimated_clv** | DECIMAL | `Monthly Charges Ã— Tenure` | Customer Lifetime Value |
| **total_services** | INT | Count de servicios='Yes' | Servicios contratados |

#### `telco_segment_kpis` (180 registros)
**PropÃ³sito**: KPIs por segmento (Contract Ã— Tenure Ã— ARPU)

**Algoritmo de SegmentaciÃ³n**:
```python
segment_kpis = df.groupby(['Contract', 'Tenure_Segment', 'ARPU_Segment']).agg({
    'Churn_Binary': ['sum', 'mean', 'count'],
    'MonthlyCharges': 'mean',
    'TotalCharges': 'mean'
})
segment_kpis['churn_rate'] = (churned / total) Ã— 100
segment_kpis['revenue_at_risk'] = churned_count Ã— avg_monthly_charges
```

### **KPIs Calculados**
- **Churn Rate**: 26.54% (1,869 clientes)
- **ARPU**: $64.76/mes
- **Revenue at Risk**: $139,131/mes ($1.67M/aÃ±o)
- **Average CLV**: $2,280
- **Impacto de reducciÃ³n 5pp**: $314K/aÃ±o recuperados

### **Drivers de Churn Identificados**
- **Contratos Month-to-Month**: 42.7% churn (vs 11% anuales)
- **Nuevos clientes (0-12m)**: 50.4% churn
- **Fiber Optic**: 41.9% churn (problemas de servicio)
- **Electronic Check**: Mayor tasa de churn por mÃ©todo de pago

---

## ğŸ”’ 4. FRAUD DETECTION - Modelo de Datos (COMPACTADO)

### **DesafÃ­o: CompactaciÃ³n de 7.48M a 1.69M registros**

**Problema**: Dataset de 750 MB excede lÃ­mite de Supabase (500 MB)  
**SoluciÃ³n**: Sampling estratÃ©gico + agregaciones pre-calculadas

### **Algoritmo de CompactaciÃ³n**

```python
# PASO 1: Conservar TODAS las transacciones fraudulentas
frauds = df[df['is_fraud'] == True]  # 1,494,719 registros (100%)

# PASO 2: Sampling estratificado de legÃ­timas
legit_sample = legit.groupby([
    'country', 
    'merchant_category', 
    'channel', 
    'card_type'
]).apply(lambda x: x.sample(
    n=proportional_size,
    random_state=42
))  # ~200,000 registros (3.3% de legÃ­timas)

# PASO 3: Combinar
compact_df = pd.concat([frauds, legit_sample])
# Resultado: 1.69M registros (-77.4%)
# TamaÃ±o: 152 MB (-79.7%)
```

### **Tablas Implementadas**

#### `fraud_transactions` (1.69M registros)
**PropÃ³sito**: Transacciones con features para ML

| Campo | Tipo | CÃ¡lculo/Origen | DescripciÃ³n |
|-------|------|----------------|-------------|
| transaction_id | TEXT | Original | ID Ãºnico |
| customer_id | TEXT | Original | ID del cliente |
| date | DATE | Parsed | Fecha de transacciÃ³n |
| hour | SMALLINT | Extracted | Hora del dÃ­a (0-23) |
| merchant_category | TEXT | Original | CategorÃ­a (Retail, Travel, etc.) |
| amount | DECIMAL | Original | Monto de transacciÃ³n |
| country | TEXT | Original | PaÃ­s de la transacciÃ³n |
| card_type | TEXT | Original | Tipo de tarjeta |
| channel | TEXT | Original | Canal (web, mobile, pos) |
| card_present | SMALLINT | Original â†’ Int | 1 si tarjeta fÃ­sica presente |
| distance_from_home | SMALLINT | Original | 0=casa, 1=lejos |
| high_risk_merchant | SMALLINT | Original â†’ Int | 1 si comerciante riesgoso |
| **velocity_num_trans** | INT | Parsed from JSON | # transacciones Ãºltima hora |
| **velocity_total_amount** | DECIMAL | Parsed from JSON | $ gastado Ãºltima hora |
| **velocity_unique_merchants** | SMALLINT | Parsed from JSON | # comerciantes distintos |
| is_fraud | SMALLINT | Original â†’ Int | 1=fraude, 0=legÃ­tima |

**Parsing de Velocity Metrics**:
```python
# Original: {'num_transactions': 1197, 'total_amount': 33498.56, ...}
velocity_data = df['velocity_last_hour'].apply(ast.literal_eval)
df['velocity_num_trans'] = velocity_data.apply(lambda x: x.get('num_transactions'))
df['velocity_total_amount'] = velocity_data.apply(lambda x: x.get('total_amount'))
# ... etc
```

#### `fraud_daily_kpis` (30 registros)
**PropÃ³sito**: KPIs diarios para dashboards

**Algoritmo de AgregaciÃ³n**:
```python
daily_kpis = df.groupby('date').agg({
    'transaction_id': 'count',
    'amount': ['sum', 'mean', 'median'],
    'is_fraud': ['sum', 'mean'],
    'customer_id': 'nunique',
    'merchant': 'nunique'
})
daily_kpis['fraud_rate'] = (fraud_count / total_transactions) Ã— 100
daily_kpis['fraud_amount'] = sum(amount WHERE is_fraud=1)
```

#### `fraud_merchant_kpis` (105 registros)
**PropÃ³sito**: Scoring de riesgo por comerciante

**Algoritmo de Risk Scoring**:
```python
merchant_kpis = df.groupby(['merchant', 'merchant_category']).agg({
    'is_fraud': ['sum', 'mean', 'count'],
    'amount': ['sum', 'mean']
})
merchant_kpis['fraud_rate'] = (fraud_count / total_transactions) Ã— 100

# Risk Level Classification
def get_risk_level(fraud_rate):
    if fraud_rate >= 50: return 'critical'
    elif fraud_rate >= 30: return 'high'
    elif fraud_rate >= 15: return 'medium'
    else: return 'low'
```

#### `fraud_country_kpis` (12 registros)
**PropÃ³sito**: AnÃ¡lisis geogrÃ¡fico de fraude

#### `fraud_hourly_patterns` (24 registros)
**PropÃ³sito**: Patrones temporales (hora del dÃ­a)

### **KPIs y SeÃ±ales de Fraude Calculados**

**MÃ©tricas Globales**:
- **Fraud Rate**: 19.97% (1.49M fraudulentas)
- **Total Transacciones**: 7.48M
- **Balance**: Ratio 1:4 (bueno para ML)

**SeÃ±ales Fuertes de Fraude** (correlaciÃ³n detectada):
- **Card Present = True**: 100% fraude âš ï¸ (todas las transacciones fÃ­sicas)
- **Distance from Home = 1**: 56.78% fraude
- **Card Not Present**: 12.35% fraude
- **High Risk Merchant**: 19.99% fraude

**Top Patrones Detectados**:
- Nigeria: Mayor volumen de fraude (849K transacciones)
- Healthcare: CategorÃ­a con mÃ¡s fraudes
- Hora 18:00: Peak de transacciones
- Web channel: 61% de transacciones

---

## ğŸ§® ALGORITMOS Y TÃ‰CNICAS UTILIZADAS

### **1. Data Processing & Cleaning**
- **Pandas/NumPy**: ETL y transformaciones
- **Missing Value Imputation**: TotalCharges rellenado con MonthlyCharges
- **Type Optimization**: Boolean â†’ SMALLINT, TIMESTAMP â†’ DATE + HOUR
- **Outlier Detection**: Z-score para montos anÃ³malos

### **2. Feature Engineering**
- **Temporal Features**: Year, Month, Quarter, Hour, DayOfWeek, IsWeekend
- **Binning/Discretization**: 
  - Tenure â†’ Segments (0-12, 13-24, 25-48, 48+)
  - ARPU â†’ Segments (Low, Medium, High, Premium)
  - Duration â†’ Flight length categories
- **Derived Metrics**: 
  - CLV = Monthly Charges Ã— Tenure
  - Gross Margin % = (Revenue - COGS) / Revenue
  - Price per Hour = Price / Duration
  - Efficiency Score = Direct Flight Rate / Avg Duration

### **3. Aggregation & Summarization**
- **GroupBy Operations**: Multi-dimensional agregaciones
- **Window Functions**: Rankings, cumulative sums
- **Pareto Analysis**: ContribuciÃ³n acumulada 80/20

### **4. Sampling Techniques (Fraud)**
- **Stratified Sampling**: Mantener distribuciÃ³n por paÃ­s, categorÃ­a, canal, tipo de tarjeta
- **Imbalanced Data Handling**: 
  - Conservar 100% de clase minoritaria (frauds)
  - Submuestrear clase mayoritaria (legÃ­timas)
  - Para ML: SMOTE, class_weight para balanceo

### **5. Data Compaction**
- **JSON Parsing**: Velocity metrics de string a columnas
- **Type Casting**: Reducir precision donde sea posible
- **Columnar Storage**: Parquet para storage eficiente (opcional)
- **Indexing Strategy**: Ãndices en columnas filtradas frecuentemente

### **6. KPI Calculation Methodology**

**Churn Rate**:
```
Churn Rate = (Customers who Churned / Total Customers) Ã— 100
```

**Revenue at Risk**:
```
Revenue at Risk = Î£(Monthly Charges of Churned Customers)
```

**Gross Margin %**:
```
Gross Margin % = ((Revenue - COGS) / Revenue) Ã— 100
COGS = Price per Unit Ã— 0.60 Ã— Quantity
```

**Fraud Rate**:
```
Fraud Rate = (Fraudulent Transactions / Total Transactions) Ã— 100
```

**Direct Flight Rate**:
```
Direct Flight Rate = (Flights with stops='zero' / Total Flights) Ã— 100
```

**Customer Lifetime Value**:
```
CLV = ARPU Ã— Tenure (months)
```

---

## ğŸ“Š VIEWS & ANALYTICAL FUNCTIONS

### **Vistas Materializadas Implementadas**

```sql
-- Vista: Resumen Ejecutivo Retail
CREATE MATERIALIZED VIEW v_retail_executive_summary AS
SELECT 
    SUM(total_amount) as total_revenue,
    SUM(gross_profit) as total_profit,
    AVG(gross_margin_pct) as avg_margin,
    COUNT(*) as total_transactions,
    COUNT(DISTINCT customer_id) as unique_customers,
    SUM(total_amount) / COUNT(*) as aov
FROM retail_transactions;

-- Vista: Top Comerciantes de Riesgo (Fraud)
CREATE VIEW v_fraud_high_risk_merchants AS
SELECT merchant, merchant_category, fraud_rate, risk_level
FROM fraud_merchant_kpis
WHERE risk_level IN ('high', 'critical')
ORDER BY fraud_rate DESC;

-- Vista: SeÃ±ales de Fraude
CREATE VIEW v_fraud_signals AS
SELECT 
    'Card Present' as signal,
    AVG(CASE WHEN card_present = 1 THEN is_fraud END) Ã— 100 as fraud_rate
FROM fraud_transactions
WHERE card_present = 1;
```

---

## ğŸ—‚ï¸ Estructura Final del Proyecto

```
data_driven/
â”œâ”€â”€ ğŸ“„ CSV Originales (3)
â”‚   â”œâ”€â”€ retail_sales_dataset.csv
â”‚   â”œâ”€â”€ airlines_flights_data.csv
â”‚   â”œâ”€â”€ WA_Fn-UseC_-Telco-Customer-Churn.csv
â”‚   â””â”€â”€ synthetic_fraud_data.csv (7.48M)
â”œâ”€â”€ ğŸ“„ CSV Procesados (11)
â”‚   â”œâ”€â”€ processed_retail_transactions.csv
â”‚   â”œâ”€â”€ processed_retail_monthly_kpis.csv
â”‚   â”œâ”€â”€ processed_airlines_flights.csv
â”‚   â”œâ”€â”€ processed_airlines_route_kpis.csv
â”‚   â”œâ”€â”€ processed_telco_customers.csv
â”‚   â”œâ”€â”€ processed_telco_segment_kpis.csv
â”‚   â”œâ”€â”€ processed_fraud_transactions.csv (compactado)
â”‚   â”œâ”€â”€ processed_fraud_daily_kpis.csv
â”‚   â”œâ”€â”€ processed_fraud_merchant_kpis.csv
â”‚   â”œâ”€â”€ processed_fraud_country_kpis.csv
â”‚   â””â”€â”€ processed_fraud_hourly_patterns.csv
â”œâ”€â”€ ğŸ Scripts Python (8)
â”‚   â”œâ”€â”€ analyze_datasets.py
â”‚   â”œâ”€â”€ process_retail.py
â”‚   â”œâ”€â”€ process_airlines.py
â”‚   â”œâ”€â”€ process_telco.py
â”‚   â”œâ”€â”€ analyze_fraud_data.py
â”‚   â”œâ”€â”€ compact_fraud_data.py
â”‚   â””â”€â”€ upload_to_supabase.py
â”œâ”€â”€ ğŸ—„ï¸ SQL Schemas (2)
â”‚   â”œâ”€â”€ supabase_schemas.sql (Retail, Airlines, Telco)
â”‚   â””â”€â”€ fraud_schemas.sql (Fraud Detection)
â”œâ”€â”€ ğŸ“– DocumentaciÃ³n (4)
â”‚   â”œâ”€â”€ readme.md (este archivo)
â”‚   â”œâ”€â”€ data_dictionary.md
â”‚   â”œâ”€â”€ executive_summary.md
â”‚   â””â”€â”€ FRAUD_COMPACTION_STRATEGY.md
â””â”€â”€ âš™ï¸ ConfiguraciÃ³n
    â”œâ”€â”€ .env.example
    â””â”€â”€ requirements.txt
```

---

## ğŸ“ TamaÃ±o y OptimizaciÃ³n

| Dataset | Registros | TamaÃ±o Original | TamaÃ±o Final | ReducciÃ³n |
|---------|-----------|-----------------|--------------|----------|
| Retail | 1,000 | 0.1 MB | 0.1 MB | 0% |
| Airlines | 300,153 | 50 MB | 50 MB | 0% |
| Telco | 7,043 | 2 MB | 2 MB | 0% |
| **Fraud** | **7.48M â†’ 1.69M** | **750 MB** | **152 MB** | **79.7%** |
| **TOTAL** | **9.48M â†’ 2.0M** | **802 MB** | **204 MB** | **74.6%** |

**Espacio en Supabase**: 204 MB / 500 MB (41% usado, 296 MB libres)

---

## ğŸ“Œ Nota de portafolio (cÃ³mo venderlo)
Este proyecto estÃ¡ diseÃ±ado para roles "business-driven data":
- **BI / Analytics**: Dashboards ejecutivos, KPIs accionables
- **Strategy**: AnÃ¡lisis Pareto, segmentaciÃ³n, priorizaciÃ³n
- **FP&A**: Forecasting, ROI, business cases
- **Revenue/Retention**: Churn analysis, CLV optimization
- **Risk/Fraud**: ML fraud detection, risk scoring
- **Data Engineering**: ETL pipelines, data compaction, optimization

Demuestra ejecuciÃ³n end-to-end: datos â†’ limpieza â†’ features â†’ KPIs â†’ insights â†’ decisiones â†’ dashboards.
